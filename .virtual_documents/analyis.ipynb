import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
import os


from sklearn.preprocessing import StandardScaler


train = pd.read_csv("train.csv", index_col="id")
test = pd.read_csv("test.csv", index_col="id")
train.head()


train.describe().T[["count","mean","50%","std","min","max"]].round({"mean":3,"std":3})


train.info()


X_columns = ["Age","BP","Max HR","Cholesterol","ST depression"]

rows, cols = 5, 3
plt.figure(figsize=(15, 20))

for i, col in enumerate(X_columns[:rows*cols]):
    plt.subplot(rows, cols, i + 1)
    sns.histplot(
        data=train,
        x=col,
        hue="Heart Disease",
        bins=15
    )
    plt.title(col)

plt.tight_layout()
plt.savefig("hist_plot.png")


X_columns = ["Age","BP","Max HR","Cholesterol","ST depression"]

rows, cols = 5, 3
plt.figure(figsize=(15, 20))

for i, col in enumerate(X_columns[:rows*cols]):
    plt.subplot(rows, cols, i + 1)
    sns.kdeplot(
        data=train,
        x=col,
        hue="Heart Disease",
    )
    plt.title(col)

plt.tight_layout()
plt.savefig("kde_plot.png")
plt.show()


X_columns = train.drop(columns=["Age","BP","Max HR","Cholesterol","ST depression"]).columns

rows, cols = 5, 3
plt.figure(figsize=(15, 20))

for i, col in enumerate(X_columns[:rows*cols]):
    plt.subplot(rows, cols, i + 1)
    sns.countplot(
        data=train,
        x=col,
        hue="Heart Disease",
    )
    plt.title(col)

plt.tight_layout()
plt.savefig("countplot.png")
plt.show()





train["Heart Disease"] = train["Heart Disease"].map({
    "Absence": 0,
    "Presence": 1
})



train[train["Heart Disease"] == 1]["Age"].shape


train[train["Heart Disease"] == 1]["Age"].std()



train["Heart Disease"].value_counts(normalize=True)






train.groupby("Heart Disease")[["Age","BP","Max HR","Cholesterol","ST depression"]].mean()







num_cols = ["Age","BP","Max HR","Cholesterol","ST depression"]

cat_cols = train.drop(columns=num_cols + ["Heart Disease"]).columns

for col in cat_cols:
    print("\n", col)
    print(pd.crosstab(train[col], train["Heart Disease"], normalize="index"))



plt.figure(figsize=(12,10))
cor = train.corr()
sns.heatmap(cor, cmap="coolwarm", annot=True)








# instend of label encoding we need to onehot encode the features
train_onehot = pd.get_dummies(train, columns=cat_cols, drop_first=True).astype(int)
train_onehot.head()


# Noramlize features 

num_cols = ["Age","BP","Max HR","Cholesterol","ST depression"]

scaler = StandardScaler()

train_onehot[num_cols] = scaler.fit_transform(train_onehot[num_cols])


# split in in x and y
X = train_onehot.drop(columns=["Heart Disease"])
y = train_onehot["Heart Disease"]


from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

log_model = LogisticRegression(max_iter=1000)
rf_model = RandomForestClassifier()
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric="logloss")


from sklearn.model_selection import cross_val_score
scoring="roc_auc"
log_scores = cross_val_score(log_model, X, y, cv=5, scoring="roc_auc")
rf_scores = cross_val_score(rf_model, X, y, cv=5, scoring="roc_auc")
xgb_scores = cross_val_score(xgb_model, X, y, cv=5, scoring="roc_auc")

print("Logistic ROC-AUC:", log_scores.mean())
print("Random Forest ROC-AUC:", rf_scores.mean())
print("XGBoost ROC-AUC:", xgb_scores.mean())





results = []


week_features = ["FBS over 120","BP","Cholesterol"]
strong_train = train.drop(columns=week_features,axis=1)
strong_train.columns


new_cats_cols = [col for col in cat_cols if col != "FBS over 120"]
strong_train_onehot = pd.get_dummies(strong_train, columns=new_cats_cols, drop_first=True).astype(int)
strong_train_onehot.head()


num_cols = ["Age","Max HR","ST depression"]

scaler = StandardScaler()

strong_train_onehot[num_cols] = scaler.fit_transform(strong_train_onehot[num_cols])


# split in in x and y
X = strong_train_onehot.drop(columns=["Heart Disease"])
y = strong_train_onehot["Heart Disease"]


log_model = LogisticRegression(max_iter=1000)
rf_model = RandomForestClassifier()
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric="logloss")


scoring="roc_auc"
log_scores = cross_val_score(log_model, X, y, cv=5, scoring="roc_auc")
rf_scores = cross_val_score(rf_model, X, y, cv=5, scoring="roc_auc")
xgb_scores = cross_val_score(xgb_model, X, y, cv=5, scoring="roc_auc")

print("Logistic ROC-AUC:", log_scores.mean())
print("Random Forest ROC-AUC:", rf_scores.mean())
print("XGBoost ROC-AUC:", xgb_scores.mean())





# BP Clinical Binning
def bp_category(bp):
    if bp < 120:
        return 0  # Normal
    elif bp < 130:
        return 1  # Elevated
    elif bp < 140:
        return 2  # Stage 1
    elif bp < 180:
        return 3  # Stage 2
    else:
        return 4  # Crisis

train["BP_category"] = train["BP"].apply(bp_category)


# Age-Adjusted BP
train["BP_Age_ratio"] = train["BP"] / train["Age"]


# Heart Rate Efficiency
train["Theoretical_MaxHR"] = 220 - train["Age"]
train["HR_gap"] = train["Theoretical_MaxHR"] - train["Max HR"]


# Risk Intensity Score
train["Risk_score"] = (
    (train["BP"] > 140).astype(int) +
    (train["Cholesterol"] > 240).astype(int) +
    (train["FBS over 120"] == 1).astype(int)
)


# Cholesterol-BP Interaction
train["Chol_BP_product"] = train["Cholesterol"] * train["BP"]


# Severe Ischemia Flag
train["Severe_ischemia"] = (
    (train["Exercise angina"] == 1) &
    (train["ST depression"] > 2.0)
).astype(int)


# ST-Slope Interaction
train["ST_Slope_interaction"] = (
    train["ST depression"] * train["Slope of ST"]
)


# check correlation now 
plt.figure(figsize=(20,15))
cor = train.corr()
sns.heatmap(cor, cmap="coolwarm", annot=True)


cat_cols = [
    "Sex",
    "Chest pain type",
    "FBS over 120",
    "EKG results",
    "Exercise angina",
    "Slope of ST",
    "Number of vessels fluro",
    "Thallium",
    "BP_category",
    "Risk_score"  
]



train_onehot = pd.get_dummies(
    train,
    columns=cat_cols,
    drop_first=True
)


from sklearn.preprocessing import StandardScaler

num_cols = [
    "Age","BP","Max HR","Cholesterol","ST depression",
    "BP_Age_ratio","HR_gap","Chol_BP_product",
    "ST_Slope_interaction","Theoretical_MaxHR"
]


scaler = StandardScaler()
train_onehot[num_cols] = scaler.fit_transform(train_onehot[num_cols])



print(train_onehot.shape)
print(train_onehot.columns)


X = train_onehot.drop(columns=["Heart Disease"])
y = train_onehot["Heart Disease"]


rf_model = RandomForestClassifier()
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric="logloss")


scoring="roc_auc"
rf_scores = cross_val_score(rf_model, X, y, cv=5, scoring="roc_auc")
xgb_scores = cross_val_score(xgb_model, X, y, cv=5, scoring="roc_auc")

print("Random Forest ROC-AUC:", rf_scores.mean())
print("XGBoost ROC-AUC:", xgb_scores.mean())


 results = pd.DataFrame({
    "Model": [
        "Logistic (All)",
        "Random Forest (All)",
        "XGBoost (All)",
        "Logistic (Strong)",
        "Random Forest (Strong)",
        "XGBoost (Strong)",
        "Random Forest (FE)",
        "XGBoost (FE)"
    ],
    "ROC_AUC": [
        0.9529311078396198,
        0.9457540409049215,
        0.9547513557818222,
        0.9528040553750395,
        0.9324653689254456,
        0.9543247901045877,
        0.9467113132791299,
        0.9545485492671698
    ]
})

results.sort_values(by="ROC_AUC", ascending=False)





import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

X = train_onehot.drop("Heart Disease", axis=1)
X = X.astype(float)

# Add constant
X_const = sm.add_constant(X)


vif_data = pd.DataFrame()
vif_data["Feature"] = X_const.columns
vif_data["VIF"] = [
    variance_inflation_factor(X_const.values, i)
    for i in range(X_const.shape[1])
]

vif_data = vif_data.sort_values("VIF", ascending=False)

print(vif_data)


# Start from one-hot encoded dataset
X = train_onehot.drop("Heart Disease", axis=1)
y = train_onehot["Heart Disease"]

# List features to remove (based on VIF)
cols_to_remove = [
    "Theoretical_MaxHR",
    "HR_gap",
    "Chol_BP_product",
    "BP_Age_ratio",
    "ST_Slope_interaction"
]

# Remove BP category dummies dynamically
bp_category_cols = [col for col in X.columns if "BP_category" in col]

# Final cleaned feature set
X_clean = X.drop(columns=cols_to_remove + bp_category_cols, errors="ignore")

print("Remaining features:", len(X_clean.columns))




X_vif = sm.add_constant(X_clean.astype(float))

vif_data = pd.DataFrame()
vif_data["Feature"] = X_vif.columns
vif_data["VIF"] = [
    variance_inflation_factor(X_vif.values, i)
    for i in range(X_vif.shape[1])
]

vif_data = vif_data.sort_values("VIF", ascending=False)

print(vif_data.head(15))



log_model = LogisticRegression(max_iter=1000)

scores = cross_val_score(log_model, X_clean, y, cv=5, scoring="roc_auc")

print("Logistic ROC-AUC (Cleaned):", scores.mean())


log_model.fit(X_clean, y)

coef_df = pd.DataFrame({
    "Feature": X_clean.columns,
    "Coefficient": log_model.coef_[0]
})

coef_df["Odds_Ratio"] = np.exp(coef_df["Coefficient"])

coef_df.sort_values("Odds_Ratio", ascending=False)

